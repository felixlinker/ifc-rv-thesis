%!TEX root = ../thesis.tex

\section{Background}


\subsection{Formal Verification of Specifications}
\label{sec:verify-spec}

As already introduced, the work of \citeauthor{Reid17} in \citetitle{Reid17} \cite{Reid17} focusses on verifying specifications, in particular an \gls{isa}, against high level properties.
The key motivation of his work is best summarized in the conclusion of the paper:
\begin{displaycquote}[p.88:22]{Reid17}
    Formal verification of programs is becoming more and more practical but, if the verification is to be meaningful, it must be based on correct architecture specifications for the hardware that the programs run on.
    That is, the specifications are a critical part of the Trusted Computing Base.
    Unfortunately, the size and complexity of architecture specifications is such that it seems inevitable that specifications will contain bugs and our previous work confirms this supposition.

    While it is common to debug specifications by \textit{testing} the specification, this paper proposes a different approach:
    we define a set of formal properties that should hold for the specification and we \textit{formally verify} the architecture specification satisfies these properties.
    We think of the relationship between the properties and the specification as being like the relationship between a nation's constitution and a nation's laws:
    the constitution is concise enough that everyone can read them while the laws are too large for effective review; the constitution can be used to test whether existing or proposed laws are compatible with high level goals; and the constitution is stable and changes very, very slowly.
\end{displaycquote}

These high-level properties comprise the formalization of \textcquote[p.88:2]{Reid17}{cross-cutting features}, i.e. features that describe expected high-level behavior of the architecture.
These have the downside that they are usually hard to grasp for humans since they require a thorough understanding of the system as a whole, yet are powerful since they often (indirectly) touch many aspects of the specification.
In his paper, \citeauthor{Reid17} gives a list of requirements for such cross-cutting properties:
\begin{displaycquote}[pp.88:2-3]{Reid17}
    The central design challenge we face is to create a set of properties that:
    \begin{itemize}
        \item express the major guarantees that programmers depend on;
        \item are concise so that architects can easily review and remember the entire set of properties;
        \item are stable so that architectural extensions don't invalidate large numbers of rules;
        \item and that describe the architecture differently from existing specification to reduce the risk of common-mode failure.
    \end{itemize}
\end{displaycquote}

The aspects of the conciseness and stability of the properties implement the idea of a \enquote{constitution} for an \gls{isa} which links back to the paragraphs quoted above.
The first point, however, more play a role for the practical applicability of the verification process.
If properties, the architecture is verified against, express \enquote{major guarantess that programmers \textins{can} depend} on, they can be used to aid in usage of the platform, e.g. by \gls{os} engineers to utilize the mechanisms of the architecture correctly and thereby to write secures \glspl{os}.
Lastly, the requirement aimed at reducing the \enquote{risk of common-mode failure} aims at other approaches of verifying specifications.
\citeauthor{Reid17} lists three commonly used ways of verifying specifications: \textcquote[p.88:2]{Reid17}{by testing specifications against existing existing implementations; by testing specifications using testsuites used to test implementations; or as a side effect of attempting to formally verify an implementation against a specification}.
These approaches, however, all face the risk of issues with the specification being undetected because the implementation and specification both are affected by the same issue, i.e. the risk of issues being undetected because of common-mode failure.
This risk can be mitigated if the properties the specification will be verified against actually expresses something \textit{new}.

\citeauthor{Reid17} applies his approach to the \gls{mclass} architecture for which not only a natural language but also a machine-readable specification is available.
The machine-readable specification of the \gls{mclass} architecture implements the function \asl{TopLevel} which implements the transition of the architecture from one state to another, i.e. a cycle of the processor.
The natural language specification on the other hand mainly consists of rules that constrain the behavior of the architecture in different situations, i.e. that constrain the transition relation between architectural states that implicitly is given by the \asl{TopLevel} function.

In principle, these two forms of specification also face the risk of common-mode failures.
In practice, however, it turned out that the natural language specification included several rules that described the behavior of the architecture on a very high-level thus meeting aforementioned requirements.
Using these rules as high-level cross-cutting properties, \citeauthor{Reid17} was able to find 12 bugs in the \gls{mclass} specification that despite prior testing had been undiscovered up to this point.

In summary, the aspects of \cite{Reid17} that are relevant for this thesis are:
\begin{enumerate}[label=\alph*)]
    \item the paper precisely argues for why the verification of specifications themselves is important; and
    \item the paper proposes clear requirements for properties to verify an \gls{isa} against.
\end{enumerate}

Additionally, the paper successfully implements this approach by partially verifying the \gls{mclass} architecture.
A downside of this work, however, is that it is limited to verifying single processor transactions only.
This lies in the nature of the properties taken from the natural language specification which only constrain single transitions of the processor.

This thesis attempts to fill this gap and proposes a new way of verifying architectural specifications which will apply to multiple processor transitions.
The work in this thesis therefore also is subject to the same requirements that have been imposed on the high-level properties in \cite{Reid17}.

\subsection{RISC-Architectures}

\subsubsection{Arm}

\subsubsection{MMIX}

\subsubsection{RISC-V}

\subsection{Processor Vulnerabilities}

\subsubsection{Processors and their Ecosystem}

\subsubsection{Common Attack Vectors}

\subsection{Information Flow Control}

\subsection{Model Checking}

Model checking is a technique that falls into the domain of formal verification.
It is one way to prove that a given system complies with a given specification.
\citeauthor{Baier08} introduce model checking in their book \citetitle{Baier08} \cite{Baier08} as:
\begin{displaycquote}[p.7ff.]{Baier08}
    \textit{Model-based} verification techniques are based on models describing the possible system behavior in a mathematically precise and unambiguous manner. \textelp{}
    This provides the basis for a whole range of verification techniques ranging from an exhaustive exploration (model checking) to experiments with a restrictive set of scenarios in the model (simulation), or in reality (testing). \textelp{}

    Model checking is a verification technique that explores all possible system states in a brute-force manner.
    Similar to a computer chess program that checks possible moves, a model checker, the software tool that performs the model checking, examines all possible system scenarios in a systematic manner.
    Int his way, it can be shown that a given system model truly satisfies a certain property. \textelp{}

    Typical properties that can be checked using model checking are of a qualitative nature:
    Is the generated result OK?,
    Can the system reach a deadlock situation, e.g., when two concurrent programs are waiting for each other and thus halting the entire system?
    But also timing properties can be checked:
    Can a deadlock occur within 1 hour after a system reset?, or, Is a response always received within 8 minutes?
\end{displaycquote}

Model checking therefore deals with two parts: Firstly, a model of some system, secondly, properties formalized on the basis of some specification.
Systems to be model checked can come in many forms.
They range from software libraries over hardware designs to embedded controllers.
The same is true for specifications.
Those can be fully fledged \textit{actual} specifications that describe the requirements to system exhaustively or more higher level properties that generally should apply to systems such as deadlock freeness as mentioned in \cite{Baier08}.

More technically, in model checking some system is taken and transformed into a model using a formal language like \gls{promela} (cf. section \ref{sec:spin}) and a specification is taken and transformed into a property usually expressed in some formal logic, e.g. \gls{ltl}.
Then it is checked via a model checker whether the system model models the formal property.
Note that \enquote{model} in this context is ambiguous.
One the one hand this term refers to the model of the system as a simplified description.
On the other hand, this term refers to the logical models-relation $ \models $ which is actually being checked.

An overview of model checking is given in figure \ref{fig:model-checking}.
There, the idea and purpose of model checking is depicted.
In summary, model checking is a technique that allows to solve the problem of ensuring that a system complies with a specification.
By translating the system into a model and the specification into a formal property this problem can be translated into \textit{checking} whether the model \textit{models} the formal property.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \tikzstyle{box}=[draw,rectangle,minimum width=2cm,minimum height=1cm,text width=2cm,align=center]
        \node[box] (system) {System};
        \node[box] (specification) [right=3cm of system] {Specification}
            edge[draw=none] node[midway] (comply) {complies with?} (system);

        \node[box] (model) [below=of system] {Model}
            edge[<-] (system);
        \node[box] (properties) [below=of specification] {Formal Property}
            edge[draw=none] node[midway] (models) {$ \models $?} (model)
            edge[<-] (specification);

        \draw[->] (comply) to (models);
    \end{tikzpicture}
    \caption{Overview on Model Checking}
    \label{fig:model-checking}
\end{figure}

\subsubsection{SPIN}
\label{sec:spin}

\gls{spin} (which stands for \textit{S}imple \textit{P}romela \textit{IN}terpreter) is a model checker which has been originally developed by Bell Labs and has been made freely available since the nineties.
As its name already suggests, it uses \gls{promela} as input language.
\textit{The Spin Model Checker: Primer and Reference Manual} which has been used as source for this section, describes \gls{spin} initially as follows:
\begin{displaycquote}[p.1]{SpinManual}
    \gls{spin} can be used to verify correctness requirements for systems of concurrently executing processes.
    The tool works by thoroughly checking either hand-built or mechanically generated models that capture the essential elements of a distributed systems design.
    If a requirement is not satisfied, \gls{spin} can produce a sample execution of the model to demonstrate this.
\end{displaycquote}

As indicated by the quote, \gls{spin} focusses heavily on the verification of distributed or parallel systems.
This is reflected not only in its input language but also in the way how properties are expressed about models.
For an introduction by example to \gls{promela}, cf. snippet \ref{snpt:spin-exm}.
\gls{promela} relies on describing systems as sets of processes that run in parallel where parallel means that each process can advance its state (generally) independent of other processes\footnote{%
    We added the restriction \textit{generally} to the claim because processes can be set up such that they deliberately wait for other processes to send them a message or set some shared state accordingly.
    However, such mechanisms where processes depend on each other always need to be implemented accordingly.
}.

In snippet \ref{snpt:spin-exm}, two processes of the same type \lstinline{user} are declared in line \ref{ln:proc}.
The idea of this snippet is to implement and verify an algorithm that grants these two processes mutually exclusive access to the critical region spanning lines \ref{ln:crit-start}-\ref{ln:crit-end}.
This is ensured by the assertion in line \ref{ln:assert} since variables in \gls{spin} are always initialized to 0 - if two processes had access to the critical region at the same time, \lstinline{cnt} would become 2 at some point.

The details of the protocol implemented in lines \ref{ln:excl-start}-\ref{ln:excl-end} the purpose of which is to grant mutual exclusive access to the critical region are not relevant for this thesis.
However, this part of code gives you an indication for how models written in \gls{promela} look like.
Besides \lstinline{if} statements and labels similar to those in C (cf. line \ref{ln:label}), \gls{promela} supports \lstinline{do}-loops to control process execution flow.
\lstinline{do}-loops run indefinitely until they are exited manually by a \lstinline{break} statement.

As data types, \gls{promela} knows three categories of them: processes, message channels and data objects.
Data objects comprise atomic data types such as \lstinline{byte}, \lstinline{bool}, \lstinline{int}, etc. as well as complex data type defined by \lstinline{typedef} that define a complex structures that have fields of data objects.
Both atomic data types and complex data types are very close to the data types of C.

\begin{figure}
    \begin{lstlisting}[
        caption={Faulty Mutual Exclusion Algorithm Implemented in \gls{promela} \cite{SpinManual}},
        label={snpt:spin-exm}
    ]
        byte cnt;
        byte x, y, z;

        active [2] proctype user() (*\label{ln:proc}*)
        {
            byte me = _pid + 1;
        again: (*\label{ln:label}*)
            x = me; (*\label{ln:excl-start}*)
            if
            :: (y == 0 || y == me) -> skip
            :: else -> goto again
            fi;

            z = me;
            if
            :: (x == me) -> skip
            :: else -> goto again
            fi;

            y = me;
            if (z = me) -> skip
            :: else -> goto again
            fi; (*\label{ln:excl-end}*)

            cnt++; (*\label{ln:crit-start}*)
            assert(cnt == 1); (*\label{ln:assert}*)
            cnt--; (*\label{ln:crit-end}*)
            goto again
        }
    \end{lstlisting}
\end{figure}

Snippet \ref{snpt:spin-output} shows the output when verifying snippet \ref{snpt:spin-exm} with \gls{spin} where we assume that latter snippet was written into a file called \lstinline{mutex_flaw.pml}.
The lines of the output are structured as follows: at the beginning of each line, you see a step index indicating the progress of all processes, then \lstinline{proc X (NAME)} indicates the process that has made progress by its ID and name.
The rest of the line \lstinline{line X ... [CODE]} shows the code executed by the process along with the corresponding line number in the source file\footnote{%
    In this case, line numbers don't fully align with snippet \ref{snpt:spin-exm} but this is not relevant here.
}.
Notice, how processes 0 and 1 progress completely independent from each other, executing code on a line by line basis where each step of any process counts as a state transition of the whole model.

\begin{figure}
    \begin{lstlisting}[
        caption={\gls{spin} Example Output \cite{SpinManual}},
        label={snpt:spin-output}
    ]
        1:  proc    1 (user) line   5 ... [x = me]
        2:  proc    1 (user) line   8 ... [(((y==0) || (y==me)))]
        3:  proc    1 (user) line  10 ... [z = me]
        4:  proc    1 (user) line  13 ... [((x = me))]
        5:  proc    0 (user) line   5 ... [x = me]
        6:  proc    0 (user) line   8 ... [(((y==0) || (y==me)))]
        7:  proc    1 (user) line  15 ... [y = me]
        8:  proc    1 (user) line  18 ... [((z = me))]
        9:  proc    1 (user) line  22 ... [cnt = cnt+1]
        10: proc    0 (user) line  10 ... [z = me]
        11: proc    0 (user) line  13 ... [((x = me))]
        12: proc    0 (user) line  15 ... [y = me]
        13: proc    0 (user) line  18 ... [((z==me))]
        14: proc    0 (user) line  22 ... [cnt = (cnt+1)]
        spin: line 223 of "mutex_flaw.pml", Error: assertion violated
        spin: text of failed assertion: assert((cnt==1))
        15: proc    0 (user) line  23 ... [assert((cnt==1))]
        spin: trail ends after 15 steps
        # processes: 2
            cnt = 2
            x = 1
            y = 1
            z = 1
        15: proc    1 (user) line  23 "mutex_flaw.pml" (state 20)
        15: proc    0 (user) line  24 "mutex_flaw.pml" (state 21)
        2 processes created
    \end{lstlisting}
\end{figure}

Assertions, however, are not the only way to express properties of \gls{promela} models for \gls{spin}.
Furthermore, there are:
\begin{itemize}
    \item Labels
    \item Never claims
    \item Trace assertions
\end{itemize}

Labels have already been introduced informally along with snippet \ref{snpt:spin-exm}.
To express properties about a model, certain types of labels can be used to give semantics to process state.
For example, you can label a certain part of process code as and end-state that might look like an idling-state to \gls{spin} by default.
% TODO: introduce deadlocks
Idling- and end-states are important to \gls{spin} when it's checking for deadlock-freeness of systems.
By default, if all processes are in an idling-state and wait for some kind of signal, this state is considered to be a deadlock.
However, in some situations it might be perfectly fine for some of the processes to idle in a specific state which should not contribute towards deadlocks.
Labeling parts of code as end-state contributes towards this.

Never claims are processes themselves.
% TODO: introduce LTL
They run like any other process but must not terminate otherwise they're regarded as failure.
This is the most complex way to express properties and in fact falls together with writing LTL properties about a model.
Therefore \gls{spin} also allows to express such never properties in LTL directly via their command line interface.

The last type of properties, trace assertions, solely deal with message passing and therefore are not relevant to this thesis.

For some, it might be obvious at this point why we decided against using \gls{spin} as the model checker of this thesis.
Its focus on verifying distributed and parallel systems is obvious and would make implementing an \gls{isa} in it very hard.
In the \textit{Primer and Reference Manual} for \gls{spin} it is written:
\begin{displaycquote}[p.33]{SpinManual}
    \textins{W}e saw that the emphasis in \gls{promela} models is placed on the coordination and synchronization aspects of a distributed system, and not on its computational aspects. \textelp{}
    The specification language we use for systems verification is therefore deliberately designed to encourage the user to abstract from the purely computational aspects of a design, and to focus on the specification of process interaction at the system level.
\end{displaycquote}

However, the \gls{isa} we will attempt to verify
\begin{enumerate*}[label=\alph*)]
    \item will most likely not include components \enquote{interacting at the system level} and
    \item will be verified on a component level, i.e. computationally as well - even in the case where multiple system level components would be given.
\end{enumerate*}

Furthermore, it is unreasonable to assume that an implementation of instructions of an \gls{isa} could be implemented in a single statement of \gls{promela}.
Yet, this should be the goal as otherwise as shown in snippet \ref{snpt:spin-output} what is regarded as state by \gls{spin} would not fall together with what is regarded as state in an \gls{isa}.
For an \gls{isa}, you typically would consider the \textit{state} of registers and memory to be the state of the \gls{isa} whilst some instruction advances this state.
However, for \gls{spin} more complex state transitions would result in us not being able to differentiate architectural states of the \gls{isa} natively since the process implementing the \gls{isa} would change state on each line of code executed.
This could be circumvented by using the \lstinline{atomic} keyword offered by \gls{promela} which lets you wrap a group of \gls{promela} statements such that they're considered as one atomic statement that advances the process state only by one step.
However, this would lead to a model consisting of one process all of its code being wrapped by one \lstinline{atomic} statement.
This would massively contradict the key idea of \gls{spin} of simple models being \textit{abstracted} from computationally complex code.
In this case, we'd have skipped the whole step of abstracting from some computational model that has distributed components running in parallel.
Not only could this lead to performance issues, it also can be safely assumed that the work for this thesis would be cumbersome and might stumble over obstacles induced by abusing \gls{spin}.

\subsubsection{\muZ{} \& \gls{spacer}}

\muZ{} is a \gls{datalog}-engine that provides querying fixed points with constraints and has been proposed in \cite{Hoder11}.
According to \textit{An Introduction to Database Systems} \cite[p.790ff]{Date00}, \gls{datalog} is a descriptive and querying language that originated in the field of database management systems.
At its core, \gls{datalog} programs are sets of \textit{rules} that combine predicates only using variables and constants to Horn-clauses, i.e. disjunctions with one positive literal at maximum.
For example, consider the following rule $ \pi_0 $ which expresses the transitivity of a predicate $ P $:
\begin{equation*}
    \pi_0 := P(a, b) \land P(b, c) \Rightarrow P(a, c)
\end{equation*}
Such programs are then used to \textit{deduct} facts from the set of rules.
To illustrate what this means, we introduce two other rules.
\begin{align*}
    \pi_1 := & P(0, 1) \Rightarrow \top \\
    \pi_2 := & P(a, b) \Rightarrow P(a + 1, b + 1)
\end{align*}
The \gls{datalog}-program $ \Pi := \{ \pi_0, \pi_1, \pi_2 \} $ now induces the relation $ < $ on all natural numbers including $ 0 $.
$ \pi_1 $ sets up a start of induction which, stating that $ 0 $ is smaller than $ 1 $ which is generalized for all natural numbers by $ \pi_2 $.

Whether or not this exact \gls{datalog}-program is supported by a given \gls{datalog}-engine depends on the extensions implemented by the respective engine.
Our example relies on an extension for scalar operators since we use the $ + $ operator.

As briefly mentioned, \gls{datalog} also supports queries.
\gls{datalog} queries comprise only one predicate and a special head $ ? $.
\begin{equation*}
    q_0 := P(0, x) \Rightarrow \; ?
\end{equation*}
The result to a query is the set of all values for each variable that make the predicate true.
In this case, the result set would be $ \{ 1, 2, 3, \dots \} $.
If no variables but only constants are given in the query, \gls{datalog} simply determines whether the given predicate can be derived for the given constants.

\muZ{} now is a \gls{datalog}-engine that comes as part of the SMT solver z3 \cite{Moura08} and adds support for expressing Horn-clauses to it.
For example, consider the implementation of the program $ \Pi $ for \muZ{} as depicted in snippet \ref{snpt:muz-exm}.
This example yields \smt{sat} as result when executed, meaning, that \smt{goal} can be derived from the rules at hand.

\begin{figure}
    \begin{lstlisting}[
        language=SMT2,
        caption={Implementation of $ \Pi $ for \muZ{}},
        label={snpt:muz-exm}
    ]
        (declare-var a Int)
        (declare-var b Int)
        (declare-var c Int)

        (declare-rel ge (Int Int))
        (declare-rel goal ())

        (rule (ge 0 1))                     ; pi_1
        (rule (=>   (ge a b)                ; pi_2
                    (ge (+ a 1) (+ b 1))))
        (rule (=>   (and (ge a b) (ge b c)) ; pi_0
                    (ge a c)))

        (rule (=>   (ge 10 15)
                    goal))
        (query goal)
    \end{lstlisting}
\end{figure}

\gls{spacer} \cite{Komuravelli13} is an algorithm that combines two widely implemented approaches of currently available formal verification tools: \gls{2bmc} and \gls{cegar}.
In its implementation, \gls{spacer} uses \muZ{} as a backend.
The paper describes those techniques as follows:
\begin{displaycquote}[p.1f]{Komuravelli13}
    The key idea of \gls{2bmc} is to iteratively construct an under-approximation \textins{(or refinement)} $ U $ of the target program $ P $ by unwinding its transition relation and check whether $ U $ is safe using \gls{bmc}.
    If $ U $ is unsafe, so is $ P $.
    Otherwise, a proof $ \pi_U $ is produced explaining \textit{why} $ U $ is safe.
    Finally, $ \pi_U $ is generalized (if possible) to a safety proof of $ P $.
    \textelp{}

    Thea idea of \textins{\gls{cegar}} is to iteratively construct, verify, and refine an abstraction (\textit{i.e.}, an over-approximation) of $ P $ based on abstract counterexamples.
\end{displaycquote}

The key to understand how \gls{spacer} works is to understand what under- or over-approximation of programs are.
For the sake of brevity, consider the program $ P $ as depicted in snippet \ref{snpt:spacer-p}.
$ P $ performs an integer division with remainder for a natural number \lstinline{n} by a divisor \lstinline{div} storing the results in \lstinline{ratio} and \lstinline{mod}.
Intuitively speaking, $ \hat{P} $ is an under-approximation (or abstraction) of $ P $ if for every part of $ P $ there is a corresponding part of $ \hat{P} $ whose effects are logically entailed by the original part.
On the other hand, $ P $ is an over-approximation of $ \hat{P} $.
An example for an over-approximation of $ P $ in snippet \ref{snpt:over-p} and an example for an under-approximation of in snippet \ref{snpt:under-p}.
For abstracting $ P $ to $ \hat{P} $ a couple of lines were removed whilst all other were left untouched.

\begin{figure}
    \centering
    \begin{minipage}{.45\linewidth}
        \begin{lstlisting}[
            linewidth=0.9\linewidth,
            caption={Program $ P $},
            label={snpt:spacer-p}
        ]
            n = abs(input());
            div = abs(input());
            ratio = 0;
            mod = 0;
            n = n - div;
            while (0 <= n) {
                ratio++;
                n = n - div;
            }
            mod = div + n;
        \end{lstlisting}
    \end{minipage}

    \begin{minipage}[t]{.45\linewidth}
        \begin{lstlisting}[
            linewidth=0.9\linewidth,
            caption={Refinement $ \bar{P} $},
            label={snpt:under-p}
        ]
            n = abs(input());
            div = abs(input());
            ratio = 0;
            mod = 0;
            n = n - div;
            if (0 <= n) {
                ratio++;
                n = n - div;
                if (0 <= n) {
                    (*\dots*)
                }
            }
        \end{lstlisting}
    \end{minipage}\hspace{0.1\linewidth}%
    \begin{minipage}[t]{.45\linewidth}
        \begin{lstlisting}[
            linewidth=0.9\linewidth,
            caption={Abstraction $ \hat{P} $},
            label={snpt:over-p},
            showlines=true
        ]
        n = abs(input());
        div = abs(input());
        ratio = 0;
        mod = 0;

        while (0 <= n) {

            n = n - div;
        }

        \end{lstlisting}
    \end{minipage}
\end{figure}

With this example at hand it can more easily be seen how \gls{2bmc} and \gls{cegar} relate to under- and over-approximations respectively.
Assume that we want to prove that always \lstinline{0 <= ratio}.
In the case of the refinement $ \bar{P} $ in snippet \ref{snpt:under-p} we could follow the idea of \gls{2bmc} and iteratively check whether \lstinline{0 <= ratio} for finite traces of $ P $.
If a counterexample is found in $ \bar{P} $ it is obvious that this counterexample must also apply to $ P $.
However, if no counterexample is found but a proof for the property at hand can be constructed one can only hope that this proof can be generalized to $ P $.

On the other hand, the converse holds for over-approximations, i.e. abstractions.
If a property is checked for the abstraction $ \hat{P} $ of $ P $ and a positive result in form of a correctness proof is given this proof will apply to $ P $ as well.
However, when a counterexample is given, it is unclear whether it applies to $ P $.
If the counterexample does not apply to $ P $, the idea of \gls{cegar} \cite{Clark00} is to refine the abstraction at hand in such a way that the counterexample does not apply to it anymore and re-iterate on the verification process.

\gls{spacer} uses both of these techniques to prove safety properties of programs expressed in \gls{datalog} using \muZ{}.
These safety properties are expressed using custom relations in \gls{datalog} such as \smt{goal} in snippet \ref{snpt:muz-exm}.
\gls{spacer} then tries to construct a proof why the property at hand cannot be derived from the axioms of the \gls{datalog} program or it gives a counterexample illustrating a possible weakness of the program.
In theory, \gls{datalog} could be used for the implementation of this thesis.
A single predicate could be used to simulate the architectural state of the instruction set architecture at hand whilst each instruction could be modelled via a separate rule.

However, whereas the limiting factor for \gls{spin} was the modelling language, the limiting factor for \gls{spacer} is the output of the tool.
In case of a property failing to be verified, no counterexamples are given but \gls{spacer} simply outputs there is an trace of the program for which a given property does not hold but without specifying the trace itself.
\muZ{} itself can be configured to give a more detailed result however that would still not include a \textit{trace of derivations}.
For \gls{spacer} or \muZ{} to be usable in the context of this thesis, these tools would need to output a log of variable values or steps taken when applying rules.

\subsubsection{nuXmv}
